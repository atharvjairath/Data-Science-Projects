{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [] # an element of X is represented as (filename,text)\n",
    "Y = [] # an element of Y represents the newsgroup category of the corresponding X element\n",
    "for category in os.listdir('20_newsgroups'):\n",
    "    for document in os.listdir('20_newsgroups/'+category):\n",
    "        with open('20_newsgroups/'+category+'/'+document, \"rb\") as f:\n",
    "            X.append((document,f.read()))\n",
    "            Y.append(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of common english words which should not affect predictions\n",
    "stopwords = ['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone',\n",
    "             'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount',\n",
    "             'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around',\n",
    "             'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before',\n",
    "             'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both',\n",
    "             'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'con', 'could', 'couldnt', 'cry', 'de',\n",
    "             'describe', 'detail', 'did', 'do', 'does', 'doing', 'don', 'done', 'down', 'due', 'during', 'each', 'eg',\n",
    "             'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone',\n",
    "             'everything', 'everywhere', 'except', 'few', 'fifteen', 'fify', 'fill', 'find', 'fire', 'first', 'five', 'for',\n",
    "             'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had',\n",
    "             'has', 'hasnt', 'have', 'having', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon',\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed',\n",
    "             'interest', 'into', 'is', 'it', 'its', 'itself', 'just', 'keep', 'last', 'latter', 'latterly', 'least', 'less',\n",
    "             'ltd', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly',\n",
    "             'move', 'much', 'must', 'my', 'myself', 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine',\n",
    "             'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once',\n",
    "             'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own',\n",
    "             'part', 'per', 'perhaps', 'please', 'put', 'rather', 're', 's', 'same', 'see', 'seem', 'seemed', 'seeming',\n",
    "             'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'so', \n",
    "             'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'system',\n",
    "             't', 'take', 'ten', 'than', 'that', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there',\n",
    "             'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thickv', 'thin', 'third', 'this',\n",
    "             'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward',\n",
    "             'towards', 'twelve', 'twenty', 'two', 'un', 'under', 'until', 'up', 'upon', 'us', 'very', 'via', 'was', 'we',\n",
    "             'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby',\n",
    "             'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom',\n",
    "             'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself',\n",
    "             'yourselves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('76049',\n",
       " b'Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv.cs.cmu.edu!fs7.ece.cmu.edu!europa.eng.gtefsd.com!howland.reston.ans.net!zaphod.mps.ohio-state.edu!rphroy!caen!kuhub.cc.ukans.edu!wsuhub.uc.twsu.edu!krschimm\\nNewsgroups: misc.forsale\\nSubject: Amplifiers and Speakers\\nMessage-ID: <1993Apr18.224905.1407@wsuhub.uc.twsu.edu>\\nFrom: krschimm@wsuhub.uc.twsu.edu (Karl Schimmel)\\nDate: 18 Apr 93 22:49:05 CST\\nOrganization: Wichita State University, Wichita, Ks\\nLines: 27\\n\\nFOR SALE(of course)\\n\\nLinear Power model 952 IQ \\n        2 channel automotive stereo amplifier\\n        95 watts peak per channel\\n        2 ohm stable\\n        fidelity tested\\n  $100 You pay shipping\\n\\n1 Pair (two (2)) Mobile Authority woofers\\n        10 inch\\n        2 inch voice coil\\n        20 oz magnet\\n        130 watt peak power handeling\\n        4 ohms \\n  $40 for both, you pay shipping (will not sell seperatly)\\n\\nreply thru e-mail to:\\n\\nKarl R. Schimmel\\n\\nThe Wichita State University\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n%krschimm at twsuvax   krschimm@wsuhub.uc.twsu.edu  %\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n\\n\\n')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f47996ba8f3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mword_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mword_new\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword_new\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mword_new\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'"
     ]
    }
   ],
   "source": [
    "# Building a vocabulary of words from the given documents\n",
    "vocab = {}\n",
    "for i in range(len(X_train)):\n",
    "    word_list = []\n",
    "    for word in X_train[i][1].split():\n",
    "        word_new  = word.strip(string.punctuation).lower()\n",
    "        if (len(word_new)>2)  and (word_new not in stopwords):  \n",
    "            if word_new in vocab:\n",
    "                vocab[word_new]+=1\n",
    "            else:\n",
    "                vocab[word_new]=1            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting a graph of no of words with a given frequency to decide cutoff drequency\n",
    "\n",
    "num_words = [0 for i in range(max(vocab.values())+1)] \n",
    "freq = [i for i in range(max(vocab.values())+1)] \n",
    "for key in vocab:\n",
    "    num_words[vocab[key]]+=1\n",
    "plt.plot(freq,num_words)\n",
    "plt.axis([1, 10, 0, 20000])\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"No of words\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_freq = 80\n",
    "# For deciding cutoff frequency\n",
    "num_words_above_cutoff = len(vocab)-sum(num_words[0:cutoff_freq]) \n",
    "print(\"Number of words with frequency higher than cutoff frequency({}) :\".format(cutoff_freq),num_words_above_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words with frequency higher than cutoff frequency are chosen as features\n",
    "# (i.e we remove words with low frequencies as they would not be significant )\n",
    "features = []\n",
    "for key in vocab:\n",
    "    if vocab[key] >=cutoff_freq:\n",
    "        features.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To represent training data as word vector counts\n",
    "X_train_dataset = np.zeros((len(X_train),len(features)))\n",
    "# This can take some time to complete\n",
    "for i in range(len(X_train)):\n",
    "    # print(i) # Uncomment to see progress\n",
    "    word_list = [ word.strip(string.punctuation).lower() for word in X_train[i][1].split()]\n",
    "    for word in word_list:\n",
    "        if word in features:\n",
    "            X_train_dataset[i][features.index(word)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To represent test data as word vector counts\n",
    "X_test_dataset = np.zeros((len(X_test),len(features)))\n",
    "# This can take some time to complete\n",
    "for i in range(len(X_test)):\n",
    "    # print(i) # Uncomment to see progress\n",
    "    word_list = [ word.strip(string.punctuation).lower() for word in X_test[i][1].split()]\n",
    "    for word in word_list:\n",
    "        if word in features:\n",
    "            X_test_dataset[i][features.index(word)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Using sklearn's Multinomial Naive Bayes\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_dataset,Y_train)\n",
    "Y_test_pred = clf.predict(X_test_dataset)\n",
    "sklearn_score_train = clf.score(X_train_dataset,Y_train)\n",
    "print(\"Sklearn's score on training data :\",sklearn_score_train)\n",
    "sklearn_score_test = clf.score(X_test_dataset,Y_test)\n",
    "print(\"Sklearn's score on testing data :\",sklearn_score_test)\n",
    "print(\"Classification report for testing data :-\")\n",
    "print(classification_report(Y_test, Y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Multinomial Naive Bayes from scratch\n",
    "class MultinomialNaiveBayes:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # count is a dictionary which stores several dictionaries corresponding to each news category\n",
    "        # each value in the subdictionary represents the freq of the key corresponding to that news category \n",
    "        self.count = {}\n",
    "        # classes represents the different news categories\n",
    "        self.classes = None\n",
    "    \n",
    "    def fit(self,X_train,Y_train):\n",
    "        # This can take some time to complete       \n",
    "        self.classes = set(Y_train)\n",
    "        for class_ in self.classes:\n",
    "            self.count[class_] = {}\n",
    "            for i in range(len(X_train[0])):\n",
    "                self.count[class_][i] = 0\n",
    "            self.count[class_]['total'] = 0\n",
    "            self.count[class_]['total_points'] = 0\n",
    "        self.count['total_points'] = len(X_train)\n",
    "        \n",
    "        for i in range(len(X_train)):\n",
    "            for j in range(len(X_train[0])):\n",
    "                self.count[Y_train[i]][j]+=X_train[i][j]\n",
    "                self.count[Y_train[i]]['total']+=X_train[i][j]\n",
    "            self.count[Y_train[i]]['total_points']+=1\n",
    "    \n",
    "    def __probability(self,test_point,class_):\n",
    "        \n",
    "        log_prob = np.log(self.count[class_]['total_points']) - np.log(self.count['total_points'])\n",
    "        total_words = len(test_point)\n",
    "        for i in range(len(test_point)):\n",
    "            current_word_prob = test_point[i]*(np.log(self.count[class_][i]+1)-np.log(self.count[class_]['total']+total_words))\n",
    "            log_prob += current_word_prob\n",
    "        \n",
    "        return log_prob\n",
    "    \n",
    "    \n",
    "    def __predictSinglePoint(self,test_point):\n",
    "        \n",
    "        best_class = None\n",
    "        best_prob = None\n",
    "        first_run = True\n",
    "        \n",
    "        for class_ in self.classes:\n",
    "            log_probability_current_class = self.__probability(test_point,class_)\n",
    "            if (first_run) or (log_probability_current_class > best_prob) :\n",
    "                best_class = class_\n",
    "                best_prob = log_probability_current_class\n",
    "                first_run = False\n",
    "                \n",
    "        return best_class\n",
    "        \n",
    "  \n",
    "    def predict(self,X_test):\n",
    "        # This can take some time to complete\n",
    "        Y_pred = [] \n",
    "        for i in range(len(X_test)):\n",
    "        # print(i) # Uncomment to see progress\n",
    "            Y_pred.append( self.__predictSinglePoint(X_test[i]) )\n",
    "        \n",
    "        return Y_pred\n",
    "    \n",
    "    def score(self,Y_pred,Y_true):\n",
    "        # returns the mean accuracy\n",
    "        count = 0\n",
    "        for i in range(len(Y_pred)):\n",
    "            if Y_pred[i] == Y_true[i]:\n",
    "                count+=1\n",
    "        return count/len(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf2 = MultinomialNaiveBayes()\n",
    "clf2.fit(X_train_dataset,Y_train)\n",
    "Y_test_pred = clf2.predict(X_test_dataset)\n",
    "our_score_test = clf2.score(Y_test_pred,Y_test)  \n",
    "print(\"Our score on testing data :\",our_score_test)\n",
    "print(\"Classification report for testing data :-\")\n",
    "print(classification_report(Y_test, Y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Score of our model on test data:\",our_score_test)\n",
    "print(\"Score of inbuilt sklearn's MultinomialNB on the same data :\",sklearn_score_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
